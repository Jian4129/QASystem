{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSX65Gymu1Xg"
   },
   "outputs": [],
   "source": [
    "TEST = False\n",
    "DOWNLOAD = True\n",
    "COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvcS1o0IpL4z"
   },
   "source": [
    "## Elastic Search Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43tFNCmQhyMC",
    "outputId": "86fd74ef-a666-46f6-873d-e37bd44aa208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting elasticsearch\n",
      "  Downloading elasticsearch-7.16.0-py2.py3-none-any.whl (385 kB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |▉                               | 10 kB 29.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█▊                              | 20 kB 24.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██▌                             | 30 kB 11.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 40 kB 9.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████▎                           | 51 kB 5.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 61 kB 6.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 71 kB 5.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▉                         | 81 kB 6.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 92 kB 4.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 102 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 112 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 122 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 133 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 143 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▊                   | 153 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▋                  | 163 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 174 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 184 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▏               | 194 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 204 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 215 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 225 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 235 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 245 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 256 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 266 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 276 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 286 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▋       | 296 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 307 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 317 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▏    | 327 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 337 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 348 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 358 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 368 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▍| 378 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 385 kB 5.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch) (1.24.3)\n",
      "Installing collected packages: elasticsearch\n",
      "Successfully installed elasticsearch-7.16.0\n"
     ]
    }
   ],
   "source": [
    "if DOWNLOAD: \n",
    "    !pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DuKJmUQzge8V"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ckhuf3aofkMq"
   },
   "outputs": [],
   "source": [
    "if DOWNLOAD:\n",
    "    !wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-linux-x86_64.tar.gz -q\n",
    "    !tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz\n",
    "!chown -R daemon:daemon elasticsearch-7.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "isN1Ii3XbKWL",
    "outputId": "a3c1ee1b-e628-41a5-d31c-22bbc661e890"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job # 0 in a separate thread.\n"
     ]
    }
   ],
   "source": [
    "%%bash --bg\n",
    "\n",
    "sudo -H -u daemon elasticsearch-7.6.2/bin/elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BmQn3s4JgR8r"
   },
   "outputs": [],
   "source": [
    "# Sleep for few seconds to let the instance start.\n",
    "time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sPq7h9-9fcfO",
    "outputId": "1643ba41-1419-4ad8-c2bf-62a53f0e527e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root         118     116  0 22:32 ?        00:00:00 sudo -H -u daemon elasticsearch-7.6.2/bin/elasticsearch\n",
      "daemon       119     118 99 22:32 ?        00:00:30 /content/elasticsearch-7.6.2/jdk/bin/java -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dio.netty.allocator.numDirectArenas=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Djava.locale.providers=COMPAT -Xms1g -Xmx1g -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -Djava.io.tmpdir=/tmp/elasticsearch-16190466211988066045 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=data -XX:ErrorFile=logs/hs_err_pid%p.log -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m -XX:MaxDirectMemorySize=536870912 -Des.path.home=/content/elasticsearch-7.6.2 -Des.path.conf=/content/elasticsearch-7.6.2/config -Des.distribution.flavor=default -Des.distribution.type=tar -Des.bundled_jdk=true -cp /content/elasticsearch-7.6.2/lib/* org.elasticsearch.bootstrap.Elasticsearch\n",
      "daemon       212     119  0 22:33 ?        00:00:00 /content/elasticsearch-7.6.2/modules/x-pack-ml/platform/linux-x86_64/bin/controller\n",
      "root         248     246  0 22:33 ?        00:00:00 grep elasticsearch\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ps -ef | grep elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgRnjq6Bv4Kb"
   },
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PQokCYsTg2y",
    "outputId": "b7f32499-e7d5-4317-da68-8087be274fdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n",
      "/content/drive/MyDrive/Colab Notebooks/NLP_Project/src\n"
     ]
    }
   ],
   "source": [
    "if COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "\n",
    "    %cd /content/drive/MyDrive/Colab Notebooks/NLP_Project/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSmBiyWQ4hBD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "data_path = \"../data/\"\n",
    "articles_path = data_path+\"articles/articles/\"\n",
    "test_data_path = data_path+\"QAData.txt\"\n",
    "test_input_path = data_path+\"Sample_Questions_and_AnswerDocs/sample_questions.txt\"\n",
    "test_output_path = data_path+\"Sample_Questions_and_AnswerDocs/sample_questions.xlsx\"\n",
    "java_path = \"D:/JDK/jdk-13.0.1/bin/java.exe\"\n",
    "\n",
    "if TEST:\n",
    "    with open(articles_path+\"400.txt\", 'r') as file:\n",
    "        test_articles = file.read()\n",
    "        file.close()\n",
    "    test_question = 'What was the capital of the Safavid Dynasty?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vYaKkJDhmJw"
   },
   "source": [
    "## TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7r1Ygw2Y-C5w",
    "outputId": "b57b4595-759a-4ebb-d4e4-4128676e0b5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "if DOWNLOAD:\n",
    "    #environment setup\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"averaged_perceptron_tagger\")\n",
    "    nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OWcX5dHXVR2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import  defaultdict\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "def articleDir(filepath):        \n",
    "    pathDir = os.listdir(filepath)\n",
    "    return pathDir\n",
    "\n",
    "def createSentenceList(data):\n",
    "    SentenceList = []\n",
    "    SentenceList = sent_tokenize(data)\n",
    "    return SentenceList\n",
    "\n",
    "def createTokenList(data):\n",
    "    TokenList = []\n",
    "    TokenList = word_tokenize(data)\n",
    "    return TokenList\n",
    "\n",
    "def Tokenize(filepath):\n",
    "    pathDir = articleDir(filePath) \n",
    "    articleSentenceList = []\n",
    "    articleTokenList = []\n",
    "    articleSentenceList2 = []\n",
    "    articleTokenList2 = []\n",
    "    for article in pathDir:\n",
    "        if article == '.DS_Store':\n",
    "            continue\n",
    "        txtname = filepath+article\n",
    "        file = open(txtname,encoding='utf-8')\n",
    "        data = file.read()\n",
    "        sentenceList = createSentenceList(data)\n",
    "        articleSentenceList.append(sentenceList)\n",
    "        articleSentenceList2.extend(sentenceList)\n",
    "        tokenList = createTokenList(data)\n",
    "        articleTokenList.append(tokenList)\n",
    "        articleTokenList2.extend(tokenList)\n",
    "    return articleSentenceList, articleSentenceList2, articleTokenList, articleTokenList2\n",
    "    \n",
    "\n",
    "def Lemmatize(tokenlist):\n",
    "    lemmalist = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for token, tag in pos_tag(tokenlist):\n",
    "        lemma = lemmatizer.lemmatize(token, tag_map[tag[0]])\n",
    "        lemmalist.append(lemma)    \n",
    "    return lemmalist\n",
    "\n",
    "def dependencyParse(sentencelist):\n",
    "    jar_path = 'nltk/stanford-corenlp-4.2.2/stanford-corenlp-4.2.2.jar'\n",
    "    models_jar_path = 'nltk/stanford-corenlp-4.2.2-models-english.jar'\n",
    "    parser = StanfordDependencyParser(path_to_jar = jar_path, path_to_models_jar = models_jar_path)\n",
    "\n",
    "    for sentence in sentencelist:\n",
    "        result = parser.raw_parse(sentence)\n",
    "        dependency = result.__next__()\n",
    "        print (\"{:<15} | {:<10} | {:<10} | {:<15} | {:<10}\".format('Head', 'Head POS','Relation','Dependent', 'Dependent POS'))\n",
    "        print (\"-\" * 75)\n",
    "        for dep in list(dependency.triples()):\n",
    "            print (\"{:<15} | {:<10} | {:<10} | {:<15} | {:<10}\".format(str(dep[0][0]),str(dep[0][1]), str(dep[1]), str(dep[2][0]),str(dep[2][1])))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dX5BaJvH6foI"
   },
   "outputs": [],
   "source": [
    "#part of speech tag \n",
    "from nltk import tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "def tag_document(doc:str) -> list:\n",
    "    sentences = tokenize.sent_tokenize(doc)\n",
    "    tagged_sentences = list()\n",
    "    for sentence in sentences: \n",
    "        tagged_sentences.append(tag_sentence(sentence))\n",
    "    return tagged_sentences\n",
    "def tag_sentence(sentence:str) -> list:\n",
    "    words = tokenize.word_tokenize(sentence)\n",
    "    return pos_tag(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQhbA50PUQJF"
   },
   "outputs": [],
   "source": [
    "poses = [wn.NOUN, wn.VERB, wn.ADJ, wn.ADV]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Po4891FWUL5N"
   },
   "outputs": [],
   "source": [
    "def wordnetSynonyms(tokenlist):\n",
    "    synonyms = []\n",
    "    antonyms = []\n",
    "    for i in range(0, len(tokenlist)):\n",
    "        wn_word = tokenlist[i].replace(' ', '_')\n",
    "        for pos in poses:\n",
    "            for syn_set in wn.synsets(wn_word, pos):\n",
    "                for l in syn_set.lemmas():\n",
    "                    synonyms.append([])\n",
    "                    synonyms[i].append(l.name())\n",
    "                    #if l.antonyms(): \n",
    "                        #antonyms.append([])\n",
    "                        #antonyms[i].append(l.antonyms()[0].name())   \n",
    "        if set(synonyms[i])!=set():          \n",
    "            print(set(synonyms[i])) \n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUtu4tAl2gKf"
   },
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "def stem(word):\n",
    "    stem=porter.stem(word)\n",
    "    #print(stem)\n",
    "    return stem\n",
    "\n",
    "def Lemma(word):\n",
    "    lemmaword = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for token, tag in pos_tag(word):\n",
    "        lemma = lemmatizer.lemmatize(token, tag_map[tag[0]])\n",
    "        lemmaword.append(lemma)    \n",
    "    #print(lemmaword)\n",
    "    return lemmaword\n",
    "\n",
    "def Synonyms(word):\n",
    "    synonyms = []\n",
    "    wn_word = word.replace(' ', '_')\n",
    "    for pos in poses:\n",
    "        for syn_set in wn.synsets(wn_word, pos):\n",
    "            for l in syn_set.lemmas():\n",
    "                synonyms.append(l.name())\n",
    "    #print(set(synonyms))\n",
    "    return list(set(synonyms)) \n",
    "\n",
    "def Antonyms(word):\n",
    "    antonyms=[]\n",
    "    wn_word = word.replace(' ', '_')\n",
    "    for pos in poses:\n",
    "        for syn_set in wn.synsets(wn_word, pos):\n",
    "            for l in syn_set.lemmas():\n",
    "                if l.antonyms():\n",
    "                    antonyms.append(l.antonyms()[0].name())\n",
    "    return list(set(antonyms))  \n",
    "\n",
    "def Hyponyms(word):\n",
    "    hyponyms = []\n",
    "    wn_word = word.replace(' ', '_')\n",
    "    for pos in poses:\n",
    "        for syn_set in wn.synsets(wn_word, pos):\n",
    "            for l in syn_set.hyponyms():\n",
    "                hyponyms.append(l.name().split('.')[0])\n",
    "    #print(set(hyponyms))\n",
    "    return list(set(hyponyms))\n",
    "\n",
    "def Hypernyms(word):\n",
    "    hypernyms = []\n",
    "    wn_word = word.replace(' ', '_')\n",
    "    for pos in poses:\n",
    "        for syn_set in wn.synsets(wn_word, pos):\n",
    "            for l in syn_set.hypernyms():\n",
    "                hypernyms.append(l.name().split('.')[0])\n",
    "    #print(set(hypernyms))\n",
    "    return list(set(hypernyms))\n",
    "    \n",
    "def Meronyms(word):\n",
    "    meronyms = []\n",
    "    wn_word = word.replace(' ', '_')\n",
    "    for pos in poses:\n",
    "        for syn_set in wn.synsets(wn_word, pos):\n",
    "            for l in syn_set.part_meronyms():\n",
    "                meronyms.append(l.name().split('.')[0])\n",
    "    #print(set(meronyms))\n",
    "    return list(set(meronyms))\n",
    "\n",
    "def Holonyms(word):\n",
    "    holonyms = []\n",
    "    wn_word = word.replace(' ', '_')\n",
    "    for pos in poses:\n",
    "        for syn_set in wn.synsets(wn_word, pos):\n",
    "            for l in syn_set.part_holonyms():\n",
    "                holonyms.append(l.name().split('.')[0])\n",
    "    #print(set(holonyms))\n",
    "    return list(set(holonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hVSiUjk0FXE0"
   },
   "outputs": [],
   "source": [
    "def postag1(questionsentence):\n",
    "    word=word_tokenize(questionsentence)\n",
    "    return pos_tag(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byUxWd7H44Py"
   },
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     filePath=\"articles\"\n",
    "#     articleSentenceList, articleSentenceList2, articleTokenList, articleTokenList2 = Tokenize(filePath)\n",
    "#     LemmaList = Lemmatize(articleTokenList2)\n",
    "#     #print(LemmaList)\n",
    "#     #dependencyParse(articleSentenceList2)\n",
    "#     #synonym=wordnetSynonyms(articleTokenList2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiL2J09mo1Kj"
   },
   "source": [
    "## Run Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfB6i92Po5YB"
   },
   "outputs": [],
   "source": [
    "def outputSentenceAndWord(articleSentenceList,articleTokenList,LemmaList):\n",
    "    file = open('sentence.txt', 'w', encoding='utf-8')\n",
    "    file.write('sentence:'+ '\\n')\n",
    "    for sentence in articleSentenceList:\n",
    "        file.write(sentence+ '\\n')\n",
    "    file.close()\n",
    "    \n",
    "    file = open('word.txt', 'w', encoding='utf-8')\n",
    "    file.write('word:'+ '\\n')\n",
    "    for word in articleTokenList:\n",
    "        file.write(word+ '\\n')\n",
    "    file.close()\n",
    "    \n",
    "    file = open('lemma.txt', 'w', encoding='utf-8')\n",
    "    file.write('lemma:'+ '\\n')\n",
    "    for lemma in LemmaList:\n",
    "        file.write(lemma+ '\\n')\n",
    "    file.close()\n",
    "    \n",
    "    file = open('POStag.txt', 'w', encoding='utf-8')\n",
    "    file.write('POStag:'+ '\\n')\n",
    "    for sentence in articleSentenceList:\n",
    "        taglist=postag1(sentence)\n",
    "        for each in taglist:\n",
    "            file.write(each[0]+'_'+each[1]+ '   ')\n",
    "        file.write('\\n')\n",
    "    file.close()\n",
    "    \n",
    "    file = open('Hypernyms.txt', 'w', encoding='utf-8')\n",
    "    file.write('Hypernyms:'+ '\\n')\n",
    "    for word in articleTokenList:\n",
    "        file.write(word+ ':  ') \n",
    "        hypernyms=Hypernyms(word)\n",
    "        for each in hypernyms:\n",
    "            file.write(each+ '  ') \n",
    "        file.write('\\n')\n",
    "    file.close()\n",
    "    \n",
    "    file = open('Hyponyms.txt', 'w', encoding='utf-8')\n",
    "    file.write('Hyponyms:'+ '\\n')\n",
    "    for word in articleTokenList:\n",
    "        file.write(word+ ':  ') \n",
    "        hyponyms=Hyponyms(word)\n",
    "        for each in hyponyms:\n",
    "            file.write(each+ '  ') \n",
    "        file.write('\\n')\n",
    "    file.close()\n",
    "    \n",
    "    file = open('Meronyms.txt', 'w', encoding='utf-8')\n",
    "    file.write('Meronyms:'+ '\\n')\n",
    "    for word in articleTokenList:\n",
    "        file.write(word+ ':  ') \n",
    "        meronyms=Meronyms(word)\n",
    "        for each in meronyms:\n",
    "            file.write(each+ '  ') \n",
    "        file.write('\\n')\n",
    "    file.close()\n",
    "    \n",
    "    file = open('Holonyms.txt', 'w', encoding='utf-8')\n",
    "    file.write('Holonyms:'+ '\\n')\n",
    "    for word in articleTokenList:\n",
    "        file.write(word+ ':  ') \n",
    "        holonyms=Holonyms(word)\n",
    "        for each in holonyms:\n",
    "            file.write(each+ '  ') \n",
    "        file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "def outputDependencyParse(sentencelist):\n",
    "    jar_path = 'nltk/stanford-corenlp-4.2.2/stanford-corenlp-4.2.2.jar'\n",
    "    models_jar_path = 'nltk/stanford-corenlp-4.2.2-models-english.jar'\n",
    "    parser = StanfordDependencyParser(path_to_jar = jar_path, path_to_models_jar = models_jar_path)\n",
    "    file = open('DependencyParse.txt', 'w', encoding='utf-8')\n",
    "    for sentence in sentencelist:\n",
    "        result = parser.raw_parse(sentence)\n",
    "        dependency = result.__next__()\n",
    "        file.write(\"{:<15} | {:<10} | {:<10} | {:<15} | {:<10}\".format('Head', 'Head POS','Relation','Dependent', 'Dependent POS'))       \n",
    "        file.write('\\n'+\"-\" * 75+'\\n')\n",
    "        for dep in list(dependency.triples()):\n",
    "            file.write(\"{:<15} | {:<10} | {:<10} | {:<15} | {:<10}\".format(str(dep[0][0]),str(dep[0][1]), str(dep[1]), str(dep[2][0]),str(dep[2][1])))\n",
    "            file.write('\\n')\n",
    "        file.write('\\n')\n",
    "\n",
    "if TEST:\n",
    "    filePath=\"articles\"\n",
    "    articleSentenceList, articleSentenceList2, articleTokenList, articleTokenList2 = Tokenize(filePath)\n",
    "    LemmaList = Lemmatize(articleTokenList2)\n",
    "    outputSentenceAndWord(articleSentenceList2,articleTokenList2,LemmaList)\n",
    "    #outputDependencyParse(articleSentenceList2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gz4twiUPhpKb"
   },
   "source": [
    "## TASK 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQBQ3z2wpJLS"
   },
   "source": [
    "### Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aytaxqYEbLSf",
    "outputId": "b08d0233-d643-42e8-94a8-14c01ac5dc14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "config = {'host':'localhost', 'port':9200}\n",
    "es = Elasticsearch([config])\n",
    "\n",
    "# test connection\n",
    "es.ping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2RmuHycpwyQ"
   },
   "source": [
    "#### parse_record_to_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7Gx6Vwaq3Yy"
   },
   "outputs": [],
   "source": [
    "from collections import  defaultdict\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "snC2eiwmOQpv"
   },
   "outputs": [],
   "source": [
    "def parse_record_to_doc(articles_path):\n",
    "    '''\n",
    "    parse into documents\n",
    "\n",
    "    Args: \n",
    "        articles_path (str) - path of the wiki articles\n",
    "    \n",
    "    Return: \n",
    "        wiki_articles (list) - list of dicts, where each dict contains two keys: 'document_id' and 'document_text'.\n",
    "    '''\n",
    "    wiki_articles = defaultdict()\n",
    "    for item in os.listdir(articles_path):\n",
    "        if item == '.DS_Store':\n",
    "            continue\n",
    "        with open(articles_path + item, 'r') as file:\n",
    "            article_id = item.split('.')[0]\n",
    "            wiki_articles[article_id] = file.read()\n",
    "            file.close()\n",
    "    wiki_articles = [{'document_id':id, 'document_text':text}\\\n",
    "                     for id, text in wiki_articles.items()]\n",
    "    return wiki_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4JYQ6YH7dz3"
   },
   "source": [
    "#### parse_record_to_n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DzacRp6S8PJZ"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-yTEQFE6wZ6"
   },
   "outputs": [],
   "source": [
    "def createSentenceList(data):\n",
    "    SentenceList = []\n",
    "    SentenceList = sent_tokenize(data)\n",
    "    return SentenceList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mq9mUMmS6Uni"
   },
   "outputs": [],
   "source": [
    "def parse_record_to_n_gram(articles, n):\n",
    "    '''\n",
    "    parse into documents with sentence bigram\n",
    "    Args: \n",
    "        articles_path (str) - path of the wiki articles\n",
    "    Return: \n",
    "        wiki_articles (list) - list of dicts, where each dict contains three keys: 'document_id', 'document_subid' and 'document_text'.\n",
    "    '''\n",
    "    wiki_articles = defaultdict()\n",
    "    \n",
    "    article_id_list = []\n",
    "    article_subid_list = []\n",
    "    article_n_gram_list = []\n",
    "    sentence_n_grams=[]\n",
    "    for article in articles:\n",
    "        temp_dict = dict()\n",
    "        article_id = article['document_id']\n",
    "        text=article['document_text']\n",
    "        sentenceList = createSentenceList(text)\n",
    "        #creates the n-gram\n",
    "        sentence_n_grams = zip(*[sentenceList[i:] for i in range(n)])\n",
    "        sentence_n_grams = [' '.join(i) for i in sentence_n_grams]\n",
    "        \n",
    "        article_id_list.extend([article_id]*len(sentence_n_grams))\n",
    "        article_subid_list.extend([i for i in range(len(sentence_n_grams))])\n",
    "        article_n_gram_list.extend(sentence_n_grams)\n",
    "    wiki_articles = zip(article_id_list, article_subid_list, article_n_gram_list)\n",
    "    wiki_articles = [{'document_id':id, 'document_subid':subid, 'document_text':text}\\\n",
    "                     for id, subid, text in wiki_articles]\n",
    "    return wiki_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYp1DKuV8An1"
   },
   "outputs": [],
   "source": [
    "articles_by_doc = parse_record_to_doc(articles_path)\n",
    "articles_by_n_grams = parse_record_to_n_gram(articles_by_doc, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrtROeRGp2-c"
   },
   "source": [
    "#### populate_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQXO7qX7pGlC"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def populate_index(es_obj, index_name, evidence_corpus):\n",
    "    '''\n",
    "    Loads records into an existing Elasticsearch index\n",
    "\n",
    "    Args:\n",
    "        es_obj (elasticsearch.client.Elasticsearch) - Elasticsearch client object\n",
    "        index_name (str) - Name of index\n",
    "        evidence_corpus (list) - List of dicts containing data records\n",
    "    '''\n",
    "    for i, rec in enumerate(tqdm(evidence_corpus)):\n",
    "        #index_status = es_obj.index(index=index_name, id=i, body=rec)\n",
    "        try:\n",
    "            index_status = es_obj.index(index=index_name, id=i, body=rec)\n",
    "        except:\n",
    "            print(f'Unable to load document {i}.')\n",
    "            \n",
    "    n_records = es_obj.count(index=index_name)['count']\n",
    "    print(f'Succesfully loaded {n_records} into {index_name}')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MrjEYtESjYEZ",
    "outputId": "baaa731b-0f22-42c2-a1e1-8dcbc8439c59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true}"
     ]
    }
   ],
   "source": [
    "!curl -XDELETE localhost:9200/_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuK9depG4kKf"
   },
   "source": [
    "#### Populate 'doc_index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZCJWoCnlh5kL",
    "outputId": "cbe9e2cd-5537-4d01-a406-7d25d515098a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:62: DeprecationWarning: The 'body' parameter is deprecated for the 'create' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'error': {'caused_by': {'reason': '/content/elasticsearch-7.6.2/config/analysis/wordnet_synonyms.txt',\n",
       "   'type': 'no_such_file_exception'},\n",
       "  'reason': 'IOException while reading synonyms_path_path: /content/elasticsearch-7.6.2/config/analysis/wordnet_synonyms.txt',\n",
       "  'root_cause': [{'reason': 'IOException while reading synonyms_path_path: /content/elasticsearch-7.6.2/config/analysis/wordnet_synonyms.txt',\n",
       "    'type': 'illegal_argument_exception'}],\n",
       "  'type': 'illegal_argument_exception'},\n",
       " 'status': 400}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index_config = {\n",
    "#     \"settings\": {\n",
    "#         \"analysis\": {\n",
    "#             \"analyzer\": {\n",
    "#                 \"stop_stem_analyzer\": {\n",
    "#                     \"type\": \"custom\",\n",
    "#                     \"tokenizer\": \"standard\",\n",
    "#                     \"filter\":[\n",
    "#                         \"lowercase\",\n",
    "#                         \"stop\",\n",
    "#                         \"snowball\"\n",
    "#                     ]\n",
    "                    \n",
    "#                 }\n",
    "#             }\n",
    "#         }\n",
    "#     },\n",
    "#     \"mappings\": {\n",
    "#         \"dynamic\": \"strict\", \n",
    "#         \"properties\": {\n",
    "#             \"document_id\": {\"type\": \"text\", \"analyzer\": \"standard_analyzer\"},\n",
    "#             \"document_text\": {\"type\": \"text\", \"analyzer\": \"standard_analyzer\"}\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "index_config ={\n",
    "  \"settings\": {\n",
    "     \"index\" : {\n",
    "        \"analysis\" : {\n",
    "            \"analyzer\" : {\n",
    "                \"synonym\" : {\n",
    "                    \"tokenizer\" : \"whitespace\",\n",
    "                    \"filter\" : [\"synonym\"]\n",
    "                }\n",
    "            },\n",
    "            \"filter\" : {\n",
    "                \"synonym\" : {\n",
    "                    \"type\" : \"synonym\",\n",
    "                    \"format\" : \"wordnet\",\n",
    "                    \"synonyms_path\" : \"analysis/wordnet_synonyms.txt\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "     }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "     \"typename\": {\n",
    "        \"fieldname\": {\n",
    "           \"type\": \"string\",\n",
    "           \"analyzer\": \"synonym\"\n",
    "        }\n",
    "     },\n",
    "     \"dynamic\": \"strict\", \n",
    "     \"properties\": {\n",
    "         \"document_id\": {\"type\": \"text\", \"analyzer\": \"standard_analyzer\"},\n",
    "         \"document_text\": {\"type\": \"text\", \"analyzer\": \"standard_analyzer\"}\n",
    "         }\n",
    "    }\n",
    "}\n",
    "\n",
    "doc_index = 'doc_index'\n",
    "es.indices.create(index=doc_index, body=index_config, ignore=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "2c32a8552ff2492e9db7a226c883788d",
      "a1c148714c3447829c21d63049880cfd",
      "92bf43b0404f4f88bd543b6704a08723",
      "496759ed50164754953a0fd092e87b3c",
      "92234c43f8a74e2e910a67d30c61ec3f",
      "fd77bfbc94404330b591f2deb82ef463",
      "1f30b1f6ae414236a899cda0a184d803",
      "4379e2f50d1f494e8e8d89588b18f769",
      "d823635fa34448b0afc1ef3f648069f2",
      "1c8837e4ac0345caa20e3ff93bbbe6be",
      "b77f1ed1c4ca4d0380544ce34261796a"
     ]
    },
    "id": "OKR8x5v_rL2s",
    "outputId": "79587c36-6067-4181-d70e-4491462c084f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c32a8552ff2492e9db7a226c883788d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: The 'body' parameter is deprecated for the 'index' API and will be removed in a future version. Instead use the 'document' parameter. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully loaded 0 into doc_index\n"
     ]
    }
   ],
   "source": [
    "populate_index(es_obj=es, index_name=doc_index, evidence_corpus=articles_by_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOCRhfYiLkXq"
   },
   "source": [
    "#### Populate 'n_gram_index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UVzWqXFILof9",
    "outputId": "09e35b39-5871-4652-a0c4-24b9d6dc29f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: DeprecationWarning: The 'body' parameter is deprecated for the 'create' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'error': {'caused_by': {'reason': '/content/elasticsearch-7.6.2/config/analysis/wordnet_synonyms.txt',\n",
       "   'type': 'no_such_file_exception'},\n",
       "  'reason': 'IOException while reading synonyms_path_path: /content/elasticsearch-7.6.2/config/analysis/wordnet_synonyms.txt',\n",
       "  'root_cause': [{'reason': 'IOException while reading synonyms_path_path: /content/elasticsearch-7.6.2/config/analysis/wordnet_synonyms.txt',\n",
       "    'type': 'illegal_argument_exception'}],\n",
       "  'type': 'illegal_argument_exception'},\n",
       " 'status': 400}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index_config = {\n",
    "#     \"settings\": {\n",
    "#         \"analysis\": {\n",
    "#             \"analyzer\": {\n",
    "#                 \"stop_stem_analyzer\": {\n",
    "#                     \"type\": \"custom\",\n",
    "#                     \"tokenizer\": \"standard\",\n",
    "#                     \"filter\":[\n",
    "#                         \"lowercase\",\n",
    "#                         \"stop\",\n",
    "#                         \"snowball\"\n",
    "#                     ]\n",
    "                    \n",
    "#                 }\n",
    "#             }\n",
    "#         }\n",
    "#     },\n",
    "#     \"mappings\": {\n",
    "#         \"dynamic\": \"strict\", \n",
    "#         \"properties\": {\n",
    "#             \"document_id\": {\"type\": \"text\", \"analyzer\": \"standard_analyzer\"},\n",
    "#             \"document_subid\": {\"type\": \"text\", \"analyzer\": \"standard_analyzer\"},\n",
    "#             \"document_text\": {\"type\": \"text\", \"analyzer\": \"standard_analyzer\"}\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "index_config ={\n",
    "  \"settings\": {\n",
    "     \"index\" : {\n",
    "        \"analysis\" : {\n",
    "            \"analyzer\" : {\n",
    "                \"synonym\" : {\n",
    "                    \"tokenizer\" : \"whitespace\",\n",
    "                    \"filter\" : [\"synonym\"]\n",
    "                }\n",
    "            },\n",
    "            \"filter\" : {\n",
    "                \"synonym\" : {\n",
    "                    \"type\" : \"synonym\",\n",
    "                    \"format\" : \"wordnet\",\n",
    "                    \"synonyms_path\" : \"analysis/wordnet_synonyms.txt\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "     }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "     \"typename\": {\n",
    "        \"fieldname\": {\n",
    "           \"type\": \"string\",\n",
    "           \"analyzer\": \"synonym\"\n",
    "        }\n",
    "     },\n",
    "     \"dynamic\": \"strict\", \n",
    "     \"properties\": {\n",
    "         \"document_id\": {\"type\": \"text\", \"analyzer\": \"standard_analyzer\"},\n",
    "         \"document_subid\": {\"type\": \"text\", \"analyzer\": \"standard_analyzer\"},\n",
    "         \"document_text\": {\"type\": \"text\", \"analyzer\": \"standard_analyzer\"}\n",
    "         }\n",
    "    }\n",
    "}\n",
    "\n",
    "n_gram_index = 'n_gram_index'\n",
    "es.indices.create(index=n_gram_index, body=index_config, ignore=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "ec63c390d00c43f181884fe99d9cee18",
      "3dc2f1be451a475db3d0fc229a183a86",
      "da7a43c73c994dad95efe772e3aaf5a9",
      "49db0f093d8f495caa46db778134a01f",
      "52e6e3726e1740eca31a49813d5889d4",
      "bd9aa1faa2934e3b9ed4219de5a3c4cf",
      "34aa90b247ec4700921373d2eaebadc5",
      "74de64df10cb488d9058349f6a323559",
      "037e485d16884d30a9ec25a8e976bfb7",
      "c99059aef8e14ee6981fd4e96586b0ec",
      "b87efd93515944a0a5f8b9efe0846e52"
     ]
    },
    "id": "7841eRBAL6b8",
    "outputId": "62198a3d-a66a-4c1f-cc83-a3d30c545c02"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec63c390d00c43f181884fe99d9cee18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7060 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: The 'body' parameter is deprecated for the 'index' API and will be removed in a future version. Instead use the 'document' parameter. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully loaded 7060 into n_gram_index\n"
     ]
    }
   ],
   "source": [
    "populate_index(es_obj=es, index_name=n_gram_index, evidence_corpus=articles_by_n_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vj8owGOro2m"
   },
   "source": [
    "### Function for Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5Oa6yxttJ_a"
   },
   "source": [
    "#### Format test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2tFLVM1fUwM"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfbsurpVhcjt"
   },
   "outputs": [],
   "source": [
    "def load_test_data(test_data_path):\n",
    "    '''\n",
    "    Load test data. It will print out any ques-ans pair that is discarded in the process. \n",
    "\n",
    "    Args:\n",
    "        test_data_path (str) - location of the test data\n",
    "    \n",
    "    Returns:\n",
    "        test_data_res (dict) - Each entry of the dict corresponding to a list of ques-ans tuples.  \n",
    "    '''\n",
    "    with open(test_data_path, 'r') as file:\n",
    "        test_data = file.read()\n",
    "        file.close()\n",
    "\n",
    "    test_data = test_data.split('\\n')\n",
    "    test_data_res = dict()\n",
    "    for line in test_data:\n",
    "        num = re.search(r'[0-9]{1,4}', line).group(0)\n",
    "        if num in test_data_res:\n",
    "            continue\n",
    "\n",
    "        test_data_res[num] = list()\n",
    "        questions = re.findall(r'\\(.*?\\),|\\(.*?\\)\\]', line)\n",
    "        for q in questions:\n",
    "            q1 = q\n",
    "            q = re.findall(r\"\\'.*?\\'|\\\".*?\\\"\", q1)\n",
    "            if len(q) != 2:\n",
    "                print(q1)\n",
    "                print(q)\n",
    "                continue\n",
    "            q[0] = q[0][1:-1]\n",
    "            q[1] = q[1][1:-1]\n",
    "            q = tuple(q)\n",
    "            test_data_res[num].append(q)\n",
    "        #print(len(test_data_res[num]))\n",
    "    return test_data_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cY2kBm3Qv1hG",
    "outputId": "a3c9245d-d64f-41bf-ad10-406cdf8bf0a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('What two formats replaced the 78?', '33 1â„3 rpm (often just referred to as the 33 rpm),\n",
      "[\"'What two formats replaced the 78?'\"]\n",
      "('What races are considered for full citizenship in Burma?', 'national races such as Kachin, Kayah (Karenni),\n",
      "[\"'What races are considered for full citizenship in Burma?'\"]\n",
      "('What is episodic memory?', '\"which attempts to capture information such as \\'what\\', \\'when\\' and \\'where\\''),\n",
      "[\"'What is episodic memory?'\", '\\'\"which attempts to capture information such as \\\\\\'', \"', \\\\'\", \"' and \\\\'\", \"''\"]\n",
      "('What is the treatment of choice for diarrhea?', 'Oral rehydration solution (ORS),\n",
      "[\"'What is the treatment of choice for diarrhea?'\"]\n"
     ]
    }
   ],
   "source": [
    "test_data = load_test_data(test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SD6nhUS8w3Tq"
   },
   "source": [
    "#### Function for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sK-wfOgW05uq"
   },
   "outputs": [],
   "source": [
    "def evalution_ans(y_true, y_predict):\n",
    "    if y_true in y_predict:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1EM3rtL2Diz"
   },
   "outputs": [],
   "source": [
    "def evaluate_filter(filter_fn, ques_ans_pair, document_number, display_miss=False, display_correct=False):\n",
    "    '''\n",
    "    evaluate the effectiveness of a filter on a single ques_ans_pair\n",
    "\n",
    "    Args:\n",
    "        ques_ans_pair (tuple) - a ques-ans tuple\n",
    "        document_number (int) - where the answer should located\n",
    "        n (int) - number of answers the filter should return \n",
    "    Returns:\n",
    "        hit (int) - 1 if the filter's return contains the correct ans; 0 otherwise\n",
    "\n",
    "    '''\n",
    "    res = filter_fn(ques_ans_pair[0])\n",
    "\n",
    "    for ans in res:\n",
    "        if document_number == ans[0] and evalution_ans(ques_ans_pair[1], ans[1]):\n",
    "            hit = 1\n",
    "            if display_correct:\n",
    "                print_correct(document_number, ques_ans_pair, res)\n",
    "            return hit\n",
    "        else:\n",
    "            pass\n",
    "    if display_miss:\n",
    "        print_miss(document_number, ques_ans_pair, res)\n",
    "\n",
    "    hit = 0\n",
    "    return hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnuYv62y1huU"
   },
   "outputs": [],
   "source": [
    "def evaluate_all(filter_fn, test_data, display_miss=False, display_correct=False):\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    for doc_num in test_data:\n",
    "        for ques_ans_pair in test_data[doc_num]:\n",
    "            total_count += 1\n",
    "            correct_count += evaluate_filter(filter_fn, ques_ans_pair, doc_num, display_miss=display_miss, display_correct=display_correct)\n",
    "    return correct_count, total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Mc_L2I03cG6"
   },
   "source": [
    "### Pipline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bj0LF17g4Gf8"
   },
   "source": [
    "#### Extract doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9h37EFHQkc3Q"
   },
   "outputs": [],
   "source": [
    "def search_doc(es_obj, index_name, question_text, n_results):\n",
    "    '''\n",
    "    Execute an Elasticsearch query on a specified index\n",
    "    \n",
    "    Args:\n",
    "        es_obj (elasticsearch.client.Elasticsearch) - Elasticsearch client object\n",
    "        index_name (str) - Name of index to query\n",
    "        query (dict) - Query DSL\n",
    "        n_results (int) - Number of results to return\n",
    "        \n",
    "    Returns\n",
    "        res - Elasticsearch response object\n",
    "    '''\n",
    "    \n",
    "    # construct query\n",
    "    query = {\n",
    "            'match': {\n",
    "                    'document_text': question_text\n",
    "                    }\n",
    "            }\n",
    "    \n",
    "    res = es_obj.search(index=index_name, query=query, size=n_results)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-pUiGX7SpBp"
   },
   "outputs": [],
   "source": [
    "def extract_doc(question, n):\n",
    "    '''\n",
    "    Extract top n doc that have the highest similarity with the question\n",
    "\n",
    "    Args:\n",
    "        question (str) - a question string\n",
    "        n (int) - number of documents the function should return\n",
    "\n",
    "    Retures:\n",
    "        res (list) - a list of ques-ans tuples  \n",
    "    '''\n",
    "    #proper_noun = extract_proper_noun(question)\n",
    "    #question = question + ' ' + (proper_noun + ' ') * 10\n",
    "    res = search_doc(es_obj=es, index_name=doc_index, question_text=question, n_results=n)\n",
    "    res = [(hit['_source']['document_id'], hit['_source']['document_text']) for hit in res['hits']['hits']]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WS5lx5_nN1B2"
   },
   "source": [
    "#### Extract n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNfWWz_HN5FA"
   },
   "outputs": [],
   "source": [
    "def search_n_grams(es_obj, index_name, doc_range, question_text, n_results):\n",
    "    '''\n",
    "    Execute an Elasticsearch query on a specified index\n",
    "    \n",
    "    Args:\n",
    "        es_obj (elasticsearch.client.Elasticsearch) - Elasticsearch client object\n",
    "        index_name (str) - Name of index to query\n",
    "        query (dict) - Query DSL\n",
    "        n_results (int) - Number of results to return\n",
    "        \n",
    "    Returns\n",
    "        res - Elasticsearch response object\n",
    "    '''\n",
    "    \n",
    "    # construct query\n",
    "    query = {\n",
    "            'bool':{\n",
    "                    'must':[\n",
    "                        {'match': {'document_text': question_text}}    \n",
    "                        ],\n",
    "                    'filter':[\n",
    "                        {'terms':{'document_id':doc_range}}\n",
    "                        ]\n",
    "                    }\n",
    "            }\n",
    "    \n",
    "    res = es_obj.search(index=index_name, query=query, size=n_results)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "seoQNQUVsquD"
   },
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "from nltk import pos_tag\n",
    "def extract_proper_noun(sentence):\n",
    "    '''\n",
    "    Extract proper noun from a sentence\n",
    "\n",
    "    Args:\n",
    "        sentence (str) - a sentence\n",
    "\n",
    "    Returns:\n",
    "        res (str) - Subset of the sentence containing only proper noun\n",
    "    '''\n",
    "    words = tokenize.word_tokenize(sentence)\n",
    "    words = pos_tag(words)\n",
    "    proper_noun = list()\n",
    "    for w in words:\n",
    "        if w[1] == 'NNP':\n",
    "            proper_noun.append(w[0])\n",
    "    \n",
    "    res = ' '.join(proper_noun)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Doa1KN7PNGc"
   },
   "outputs": [],
   "source": [
    "def extract_n_grams(doc_range, question, n):\n",
    "    '''\n",
    "    Extract top n doc that have the highest similarity with the question\n",
    "\n",
    "    Args:\n",
    "        question (str) - a question string\n",
    "        n (int) - number of documents the function should return\n",
    "\n",
    "    Retures:\n",
    "        res (list) - a list of ques-ans tuples  \n",
    "    '''\n",
    "    #proper_noun = extract_proper_noun(question)\n",
    "    #question = question + ' ' + (proper_noun + ' ') * 10\n",
    "    res = search_n_grams(es_obj=es, index_name=n_gram_index, doc_range=doc_range, question_text=question, n_results=n)\n",
    "    res = [(hit['_source']['document_id'], hit['_source']['document_text']) for hit in res['hits']['hits']]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StMirKPaYzU7"
   },
   "source": [
    "#### Choose Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twUYEXtXY5U_"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def choose_doc(id_text_pairs):\n",
    "    doc_id = [i[0] for i in id_text_pairs]\n",
    "    c = Counter(doc_id)\n",
    "    most_common_doc = c.most_common(1)[0][0]\n",
    "\n",
    "    new_list = []\n",
    "    for id_text in id_text_pairs:\n",
    "        if id_text[0] == most_common_doc:\n",
    "            new_list.append(id_text)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFAAkvaG4ggt"
   },
   "source": [
    "#### Filter Proper Noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eo1ltn3i4pSJ"
   },
   "outputs": [],
   "source": [
    "def postag(questionsentence):\n",
    "    word=word_tokenize(questionsentence)\n",
    "    return pos_tag(word)\n",
    "\n",
    "def processTag(taglist):\n",
    "    keywordNNP=[]\n",
    "    for i in range(len(taglist)):\n",
    "        if taglist[i][1]=='NNP' or taglist[i][1]=='NNPS':\n",
    "            keywordNNP.append(taglist[i][0])\n",
    "    return keywordNNP\n",
    "\n",
    "def extractNNP(questionsentence):\n",
    "    tag_tuple = postag(questionsentence)\n",
    "    keywordNNP = set(processTag(tag_tuple))\n",
    "\n",
    "    return keywordNNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kL2pd8Ac4vho"
   },
   "outputs": [],
   "source": [
    "def filter_proper_noun(question_sentence, id_text_pairs):\n",
    "    tag_tuple = postag(question_sentence)\n",
    "    keywordNNP = set(processTag(tag_tuple))\n",
    "\n",
    "    new_list = []\n",
    "    for id_text_pair in id_text_pairs:\n",
    "        temp = word_tokenize(id_text_pair[1])\n",
    "        temp = set(temp)\n",
    "        if keywordNNP.issubset(temp):\n",
    "            new_list.append(id_text_pair)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "we1V34uh6vxQ"
   },
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    question_sentence = 'What was the capital of the Safavid Dynasty?'\n",
    "    id_text_pairs = [('400', 'Establishment of the Safavid Dynasty in 1501, converted the Iranian people from Sunni Islam to Twelver Shia Islam, and made Twelver Shia Islam the official religion of Iran. Safavid conversion of Iran from Sunnism to Shiism marked one of the most important turning points in Iranian and Muslim history. Starting in 1736 under Nader Shah, Iran reached its greatest territorial extent since the Sassanid Empire, briefly possessing what was arguably the most powerful empire at the time.'), ('400', 'In 1387, Timur ordered the complete massacre of Isfahan, reportedly killing 70,000 citizens. The Ilkhans and the Timurids soon came to adopt the ways and customs of the Iranians, choosing to surround themselves with a culture that was distinctively Iranian. By the 1500s, Ismail I from Ardabil, established the Safavid Dynasty, with Tabriz as the capital.'), ('400', 'By the 1500s, Ismail I from Ardabil, established the Safavid Dynasty, with Tabriz as the capital. Beginning with Azerbaijan, he subsequently extended his authority over all of the Iranian territories, and established an intermittent Iranian hegemony over the vast relative regions, reasserting the Iranian identity within large parts of the Greater Iran. Iran was predominantly Sunni, but Ismail instigated a forced conversion to the Shia branch of Islam, by which the Shia Islam spread throughout the Safavid territories in the Caucasus, Iran, Anatolia, and Mesopotamia.'), ('400', 'The Ilkhans and the Timurids soon came to adopt the ways and customs of the Iranians, choosing to surround themselves with a culture that was distinctively Iranian. By the 1500s, Ismail I from Ardabil, established the Safavid Dynasty, with Tabriz as the capital. Beginning with Azerbaijan, he subsequently extended his authority over all of the Iranian territories, and established an intermittent Iranian hegemony over the vast relative regions, reasserting the Iranian identity within large parts of the Greater Iran.'), ('6', 'Meanwhile, the Chinese Ming dynasty fell to the rebellion of Li Zicheng (1606–1645) in 1644, yet his short-lived Shun dynasty was crushed by the Manchu invasion and the Han Chinese general Wu Sangui (1612–1678). China Daily states that when the following Qing dynasty replaced the Ming dynasty, it merely \"strengthened administration of Tibet.\" However, Kolmaš states that the Dalai Lama was very observant of what was going on in China and accepted a Manchu invitation in 1640 to send envoys to their capital at Mukden in 1642, before the Ming collapsed.'), ('6', 'China Daily states that when the following Qing dynasty replaced the Ming dynasty, it merely \"strengthened administration of Tibet.\" However, Kolmaš states that the Dalai Lama was very observant of what was going on in China and accepted a Manchu invitation in 1640 to send envoys to their capital at Mukden in 1642, before the Ming collapsed. Dawa Norbu, William Rockhill, and George N. Patterson write that when the Shunzhi Emperor (r. 1644–1661) of the subsequent Qing dynasty invited the fifth Dalai Lama Lozang Gyatso to Beijing in 1652, Shunzhi treated the Dalai Lama as an independent sovereign of Tibet.'), ('400', 'Iran became a major contributor to the Islamic Golden Age, producing many influential scientists, scholars, artists, and thinkers. Establishment of the Safavid Dynasty in 1501, converted the Iranian people from Sunni Islam to Twelver Shia Islam, and made Twelver Shia Islam the official religion of Iran. Safavid conversion of Iran from Sunnism to Shiism marked one of the most important turning points in Iranian and Muslim history.'), ('304', 'The Shunga Empire or Shunga Empire was an ancient Indian dynasty from Magadha that controlled vast areas of the Indian subcontinent from around 187 to 78 BCE. The dynasty was established by Pushyamitra Shunga, after the fall of the Maurya Empire. Its capital was Pataliputra, but later emperors such as Bhagabhadra also held court at Besnagar, modern Vidisha in Eastern Malwa.'), ('6', 'However, Kolmaš states that the Dalai Lama was very observant of what was going on in China and accepted a Manchu invitation in 1640 to send envoys to their capital at Mukden in 1642, before the Ming collapsed. Dawa Norbu, William Rockhill, and George N. Patterson write that when the Shunzhi Emperor (r. 1644–1661) of the subsequent Qing dynasty invited the fifth Dalai Lama Lozang Gyatso to Beijing in 1652, Shunzhi treated the Dalai Lama as an independent sovereign of Tibet. Patterson writes that this was an effort of Shunzhi to secure an alliance with Tibet that would ultimately lead to the establishment of Manchu rule over Mongolia.'), ('400', 'Safavid conversion of Iran from Sunnism to Shiism marked one of the most important turning points in Iranian and Muslim history. Starting in 1736 under Nader Shah, Iran reached its greatest territorial extent since the Sassanid Empire, briefly possessing what was arguably the most powerful empire at the time. During the 19th century, Iran irrevocably lost swaths of its territories in the Caucasus which made part of the concept of Iran for centuries, to neighboring Imperial Russia.')]\n",
    "\n",
    "    #question_sentence='What latitudes does Iran lie between?'\n",
    "    #id_text_pairs=[('400','Establishment of the Safavid Dynasty in 1501, converted the Iranian people from Sunni Islam to Twelver Shia Islam, and made Twelver Shia Islam the official religion of Iran. Safavid conversion of Iran from Sunnism to Shiism marked one of the most important turning points in Iranian and Muslim history. Starting in 1736 under Nader Shah, Iran reached its greatest territorial extent since the Sassanid Empire, briefly possessing what was arguably the most powerful empire at the time.'), ('400', 'In 1387, Timur ordered the complete massacre of Isfahan, reportedly killing 70,000 citizens. The Ilkhans and the Timurids soon came to adopt the ways and customs of the Iranians, choosing to surround themselves with a culture that was distinctively Iranian. By the 1500s, Ismail I from Ardabil, established the Safavid Dynasty, with Tabriz as the capital.'), ('400', 'By the 1500s, Ismail I from Ardabil, established the Safavid Dynasty, with Tabriz as the capital. Beginning with Azerbaijan, he subsequently extended his authority over all of the Iranian territories, and established an intermittent Iranian hegemony over the vast relative regions, reasserting the Iranian identity within large parts of the Greater Iran. Iran was predominantly Sunni, but Ismail instigated a forced conversion to the Shia branch of Islam, by which the Shia Islam spread throughout the Safavid territories in the Caucasus, Iran, Anatolia, and Mesopotamia.'), ('400', 'The Ilkhans and the Timurids soon came to adopt the ways and customs of the Iranians, choosing to surround themselves with a culture that was distinctively Iranian. By the 1500s, Ismail I from Ardabil, established the Safavid Dynasty, with Tabriz as the capital. Beginning with Azerbaijan, he subsequently extended his authority over all of the Iranian territories, and established an intermittent Iranian hegemony over the vast relative regions, reasserting the Iranian identity within large parts of the Greater Iran.'), ('6', 'Meanwhile, the Chinese Ming dynasty fell to the rebellion of Li Zicheng (1606–1645) in 1644, yet his short-lived Shun dynasty was crushed by the Manchu invasion and the Han Chinese general Wu Sangui (1612–1678). China Daily states that when the following Qing dynasty replaced the Ming dynasty, it merely \"strengthened administration of Tibet.\" However, Kolmaš states that the Dalai Lama was very observant of what was going on in China and accepted a Manchu invitation in 1640 to send envoys to their capital at Mukden in 1642, before the Ming collapsed.'), ('6', 'China Daily states that when the following Qing dynasty replaced the Ming dynasty, it merely \"strengthened administration of Tibet.\" However, Kolmaš states that the Dalai Lama was very observant of what was going on in China and accepted a Manchu invitation in 1640 to send envoys to their capital at Mukden in 1642, before the Ming collapsed. Dawa Norbu, William Rockhill, and George N. Patterson write that when the Shunzhi Emperor (r. 1644–1661) of the subsequent Qing dynasty invited the fifth Dalai Lama Lozang Gyatso to Beijing in 1652, Shunzhi treated the Dalai Lama as an independent sovereign of Tibet.'), ('400', 'Iran became a major contributor to the Islamic Golden Age, producing many influential scientists, scholars, artists, and thinkers. Establishment of the Safavid Dynasty in 1501, converted the Iranian people from Sunni Islam to Twelver Shia Islam, and made Twelver Shia Islam the official religion of Iran. Safavid conversion of Iran from Sunnism to Shiism marked one of the most important turning points in Iranian and Muslim history.'), ('304', 'The Shunga Empire or Shunga Empire was an ancient Indian dynasty from Magadha that controlled vast areas of the Indian subcontinent from around 187 to 78 BCE. The dynasty was established by Pushyamitra Shunga, after the fall of the Maurya Empire. Its capital was Pataliputra, but later emperors such as Bhagabhadra also held court at Besnagar, modern Vidisha in Eastern Malwa.'), ('6', 'However, Kolmaš states that the Dalai Lama was very observant of what was going on in China and accepted a Manchu invitation in 1640 to send envoys to their capital at Mukden in 1642, before the Ming collapsed. Dawa Norbu, William Rockhill, and George N. Patterson write that when the Shunzhi Emperor (r. 1644–1661) of the subsequent Qing dynasty invited the fifth Dalai Lama Lozang Gyatso to Beijing in 1652, Shunzhi treated the Dalai Lama as an independent sovereign of Tibet. Patterson writes that this was an effort of Shunzhi to secure an alliance with Tibet that would ultimately lead to the establishment of Manchu rule over Mongolia.'), ('400', 'Safavid conversion of Iran from Sunnism to Shiism marked one of the most important turning points in Iranian and Muslim history. Starting in 1736 under Nader Shah, Iran reached its greatest territorial extent since the Sassanid Empire, briefly possessing what was arguably the most powerful empire at the time. During the 19th century, Iran irrevocably lost swaths of its territories in the Caucasus which made part of the concept of Iran for centuries, to neighboring Imperial Russia.')]\n",
    "    new_list = filter_proper_noun(question_sentence, id_text_pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i_sOHui5VV5"
   },
   "source": [
    "#### Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MINcDpfb5YNo"
   },
   "outputs": [],
   "source": [
    "def processDependencyParse(questionsentence):\n",
    "    \"\"\"\n",
    "\n",
    "    Returns:\n",
    "        head (list) - head of the sentence\n",
    "        notHead (list) - not head element of the sentence\n",
    "    \"\"\"\n",
    "    jar_path = 'nltk/stanford-corenlp-4.2.2/stanford-corenlp-4.2.2.jar'\n",
    "    models_jar_path = 'nltk/stanford-corenlp-4.2.2-models-english.jar'\n",
    "    parser = StanfordDependencyParser(path_to_jar = jar_path, path_to_models_jar = models_jar_path)\n",
    "    head=[]\n",
    "    notHead=[]\n",
    "    result = parser.raw_parse(questionsentence)\n",
    "    dependency = result.__next__()\n",
    "    for dep in list(dependency.triples()):\n",
    "        if (dep[0][0]!='What')and(dep[0][0]!='Who')and(dep[0][0]!='When'):\n",
    "          head.append(dep[0][0])\n",
    "    #print(list(set(head)))\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(questionsentence)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    #print(filtered_sentence)\n",
    "    for word in filtered_sentence:\n",
    "        if (word not in head) and (word!='When')and(word!='What')and(word!='Who')and(word!='?')and(word!=\"'s\"):\n",
    "            notHead.append(word)\n",
    "    #print(notHead)\n",
    "    return list(set(head)), notHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M5vMJGzQ9ZC_",
    "outputId": "b49f4bd0-1086-46c6-c2cc-23f8267ddb4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIszceDS9E-3"
   },
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    question_sentence = 'What was the capital of the Safavid Dynasty?'\n",
    "    #question_sentence = 'What latitudes does Iran lie between?'\n",
    "\n",
    "    head, notHead = processDependencyParse(question_sentence)\n",
    "\n",
    "    print(head)\n",
    "    print(notHead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoXlCbOZIWIZ"
   },
   "outputs": [],
   "source": [
    "def GetAllRelationalWord(word):\n",
    "    AllRelationalWord=[]\n",
    "    AllRelationalWord.append(word)\n",
    "    AllRelationalWord.append(stem(word))\n",
    "    AllRelationalWord.extend(Lemma([word]))\n",
    "    AllRelationalWord.extend(Synonyms(word))\n",
    "    AllRelationalWord.extend(Antonyms(word))\n",
    "    AllRelationalWord.extend(Hyponyms(word))\n",
    "    AllRelationalWord.extend(Hypernyms(word))\n",
    "    AllRelationalWord.extend(Meronyms(word))\n",
    "    AllRelationalWord.extend(Holonyms(word))\n",
    "    return list(set(AllRelationalWord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_HKvNpSIiik"
   },
   "outputs": [],
   "source": [
    "#test\n",
    "# AllRelationalWord=GetAllRelationalWord('name')\n",
    "# print(AllRelationalWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7Fet9qoJTMN"
   },
   "outputs": [],
   "source": [
    "def getRelativeWord(head, notHead):\n",
    "  headRelationalWord=[]\n",
    "  notHeadRelationalWord=[]\n",
    "  for headword in head:\n",
    "    headRelationalWord.append(GetAllRelationalWord(headword))\n",
    "  for notheadword in notHead:\n",
    "    notHeadRelationalWord.append(GetAllRelationalWord(notheadword))\n",
    "  #print(headRelationalWord)\n",
    "  #print(notHeadRelationalWord)\n",
    "  return headRelationalWord, notHeadRelationalWord\n",
    "\n",
    "#getRelativeWord(head, notHead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajnU0EErNNVz"
   },
   "outputs": [],
   "source": [
    "#Initial score list\n",
    "def createScoreList(new_list):\n",
    "  lenOfCandidate=len(new_list)\n",
    "  score=[]\n",
    "  for i in range(0,lenOfCandidate):\n",
    "    score.append(0)\n",
    "  #print(score)\n",
    "  return score, lenOfCandidate\n",
    "\n",
    "#createScoreList(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zrvJvpZBOZu"
   },
   "outputs": [],
   "source": [
    "def initialscore(list,lenOfCandidate):\n",
    "  score = [i for i in range(len(list))]\n",
    "  for i in range(0,lenOfCandidate):\n",
    "    score[i]=0\n",
    "  #print(score)\n",
    "  return list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPhidPbfO7l8"
   },
   "outputs": [],
   "source": [
    "def candidateRelativeWord(new_list):\n",
    "  sentencetokenlist=[]\n",
    "  stemlist=[]\n",
    "  Lemmalist=[]\n",
    "  for i in range(0,len(new_list)):\n",
    "    sentencetokenlist.append([])\n",
    "    sentencetokenlist[i].extend(word_tokenize(new_list[i][1]))\n",
    "  #print(sentencetokenlist)\n",
    "  for i in range(0,len(new_list)):\n",
    "    stemlist.append([])\n",
    "    for word in sentencetokenlist[i]:\n",
    "      #print(stem(word))\n",
    "      stemlist[i].append(stem(word))\n",
    "  #print(stemlist)\n",
    "  for i in range(0,len(new_list)):\n",
    "    Lemmalist.append([])\n",
    "    for word in sentencetokenlist[i]:\n",
    "      Lemmalist[i].extend(Lemma([word]))\n",
    "  for i in range(0,len(new_list)):\n",
    "    sentencetokenlist[i].extend(stemlist[i])\n",
    "    sentencetokenlist[i].extend(Lemmalist[i])\n",
    "  #print(sentencetokenlist)\n",
    "  return sentencetokenlist\n",
    "\n",
    "#candidateRelativeWord(new_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tivzTzMi7MO_"
   },
   "outputs": [],
   "source": [
    "def computeScore(score,lenOfCandidate,headRelationalWord,notHeadRelationalWord,sentencetokenlist):\n",
    "  initialscore(score,lenOfCandidate)\n",
    "  for i in range(0,lenOfCandidate):\n",
    "    for j in range(0,len(headRelationalWord)):\n",
    "      for word in headRelationalWord[j]:\n",
    "        if word in sentencetokenlist[i]:\n",
    "          score[i]=score[i]+2\n",
    "          break\n",
    "  #print(score)\n",
    "  #initialscore(score,lenOfCandidate)\n",
    "  for i in range(0,lenOfCandidate):\n",
    "    for j in range(0,len(notHeadRelationalWord)):\n",
    "      for word in notHeadRelationalWord[j]:\n",
    "        if word in sentencetokenlist[i]:\n",
    "          score[i]=score[i]+1\n",
    "          break\n",
    "  #print(score)\n",
    "  #initialscore(score,lenOfCandidate)\n",
    "  return score\n",
    "\n",
    "#computeScore(score,lenOfCandidate,headRelationalWord,notHeadRelationalWord,sentencetokenlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SojJ8PRDR0v"
   },
   "outputs": [],
   "source": [
    "def getBestMatchIndex(score):\n",
    "  #print(score)\n",
    "  max_val = 0\n",
    "  idx_max = 0\n",
    "  for i in range(len(score)):\n",
    "    if score[i] > max_val:\n",
    "      max_val = score[i]\n",
    "      idx_max = i\n",
    "  #print(score, max_val, idx_max)\n",
    "  return idx_max\n",
    "\n",
    "#bestIndex=getBestMatchIndex(score)\n",
    "#print(bestIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Upf_TlGK1FX"
   },
   "outputs": [],
   "source": [
    "#find the best match sentence trigram\n",
    "# BestMatchSentence=new_list[bestIndex]\n",
    "# print(BestMatchSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n60Hl3uNLErY"
   },
   "outputs": [],
   "source": [
    "# SplitBestMatch = []\n",
    "# SplitBestMatch = sent_tokenize(BestMatchSentence[1])\n",
    "# lenOfBestMatch= len(SplitBestMatch)\n",
    "# print(SplitBestMatch)\n",
    "# BestMatchsentencetokenlist=[]\n",
    "# for i in range(0,len(SplitBestMatch)):\n",
    "#   BestMatchsentencetokenlist.append([])\n",
    "#   BestMatchsentencetokenlist[i].extend(word_tokenize(SplitBestMatch[i]))\n",
    "# print(BestMatchsentencetokenlist)\n",
    "# score2=[]\n",
    "# for i in range(0,lenOfBestMatch):\n",
    "#   score2.append(0)\n",
    "# #print(score2)\n",
    "# computeScore(score2,lenOfBestMatch,headRelationalWord,notHeadRelationalWord,BestMatchsentencetokenlist)\n",
    "# bestIndexfinal=getBestMatchIndex(score2)\n",
    "# #print(bestIndexfinal)\n",
    "# Answer=[[],[]]\n",
    "# Answer[0]=BestMatchSentence[0]\n",
    "# Answer[1]=SplitBestMatch[bestIndexfinal]\n",
    "# print(Answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTmKYoTMMjJn"
   },
   "outputs": [],
   "source": [
    "# #print best match answer\n",
    "# print(Answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TD3F9G0wpBhN"
   },
   "outputs": [],
   "source": [
    "#main\n",
    "def getAnswer(new_list, question_sentence):\n",
    "  head, notHead = processDependencyParse(question_sentence)\n",
    "  headRelationalWord, notHeadRelationalWord = getRelativeWord(head, notHead)\n",
    "  score,lenOfCandidate = createScoreList(new_list)\n",
    "  sentencetokenlist = candidateRelativeWord(new_list)\n",
    "  score = computeScore(score,lenOfCandidate,headRelationalWord,notHeadRelationalWord,sentencetokenlist)\n",
    "  bestIndex = getBestMatchIndex(score)\n",
    "  BestMatchSentence=new_list[bestIndex]\n",
    "  SplitBestMatch = []\n",
    "  SplitBestMatch = sent_tokenize(BestMatchSentence[1])\n",
    "  lenOfBestMatch= len(SplitBestMatch)\n",
    "  #print(SplitBestMatch)\n",
    "  BestMatchsentencetokenlist=[]\n",
    "  for i in range(0,len(SplitBestMatch)):\n",
    "    BestMatchsentencetokenlist.append([])\n",
    "    BestMatchsentencetokenlist[i].extend(word_tokenize(SplitBestMatch[i]))\n",
    "  #print(BestMatchsentencetokenlist)\n",
    "  score2=[]\n",
    "  for i in range(0,lenOfBestMatch):\n",
    "    score2.append(0)\n",
    "  #print(score2)\n",
    "  score2 = computeScore(score2,lenOfBestMatch,headRelationalWord,notHeadRelationalWord,BestMatchsentencetokenlist)\n",
    "  bestIndexfinal=getBestMatchIndex(score2)\n",
    "  #print(bestIndexfinal)\n",
    "  Answer=[[],[]]\n",
    "  Answer[0]=BestMatchSentence[0]\n",
    "  Answer[1]=SplitBestMatch[bestIndexfinal]\n",
    "  #print(Answer)\n",
    "  return [tuple(Answer)]\n",
    "\n",
    "#answer = getAnswer(new_list, question_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lScDSahssOXL"
   },
   "outputs": [],
   "source": [
    "#print answer\n",
    "#print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJ5LdGI246ga"
   },
   "source": [
    "#### Extract Question Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tV05ui0249S5"
   },
   "outputs": [],
   "source": [
    "def extract_question_type(question):\n",
    "    question = question.upper()\n",
    "    word_list = word_tokenize(question)\n",
    "    \n",
    "    if 'WHO' in word_list:\n",
    "        return 'WHO'\n",
    "    if 'WHEN' in word_list:\n",
    "        return 'WHEN'\n",
    "    if 'WHAT' in word_list:\n",
    "        return 'WHAT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdEPIypw6ZEA"
   },
   "source": [
    "#### Check pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8z8ekPeJtnIQ"
   },
   "outputs": [],
   "source": [
    "def extract_pos_set(sentence, pos_set):\n",
    "    tag_tuple = postag(sentence)\n",
    "\n",
    "    keyword=[]\n",
    "    for i in range(len(tag_tuple)):\n",
    "        if tag_tuple[i][1] in pos_set:\n",
    "            keyword.append(tag_tuple[i][0])\n",
    "    return keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJ6c_qnH6e3o"
   },
   "outputs": [],
   "source": [
    "def check_pos(sentence, pos_set):\n",
    "    '''\n",
    "    Return True if there is pos_set in the sentence, return False otherwise\n",
    "    '''\n",
    "    pos = extract_pos_set(sentence, pos_set)\n",
    "\n",
    "    if pos:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VreQ2CK77sCJ"
   },
   "outputs": [],
   "source": [
    "def containing_pos(id_text_pairs, pos_set):\n",
    "    '''\n",
    "    filter if \n",
    "    '''\n",
    "    new_list = []\n",
    "    for id_text_pair in id_text_pairs:\n",
    "        if check_pos(id_text_pair[1], pos_set):\n",
    "            new_list.append(id_text_pair)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxyumxoGVxjG"
   },
   "source": [
    "#### Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W68BdxejYBZE"
   },
   "outputs": [],
   "source": [
    "def print_miss(document_number, ques_ans_pair, res):\n",
    "    if document_number != '400':\n",
    "        return \n",
    "    print(document_number + ' ' + str(len(res)))\n",
    "    print('miss', end=' ')\n",
    "    print(ques_ans_pair)\n",
    "    for i in res:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l99Gs3YmlZWZ"
   },
   "outputs": [],
   "source": [
    "def print_correct(document_number, ques_ans_pair, res):\n",
    "    if document_number != '400':\n",
    "        return \n",
    "    print(document_number + ' ' + str(len(res)))\n",
    "    print('hit', end=' ')\n",
    "    print(ques_ans_pair)\n",
    "    for i in res:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QT0Kn3U7V0Os"
   },
   "outputs": [],
   "source": [
    "def run_pipe(question):\n",
    "    question_type = extract_question_type(question)\n",
    "    res = extract_doc(question, 5)\n",
    "    top_n = [i[0] for i in res]\n",
    "    res = extract_n_grams(doc_range=top_n, question=question, n=10)\n",
    "\n",
    "    # if question_type == 'WHO':\n",
    "    #     res = containing_pos(id_text_pairs, {'NNP', 'NNPS'})\n",
    "    # elif question_type == 'WHEN':\n",
    "    #     res = containing_pos(id_text_pairs, {'CD'})\n",
    "\n",
    "    res = getAnswer(res, question)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2TVFmkfYTbt"
   },
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDMJRthBvbz9"
   },
   "outputs": [],
   "source": [
    "a = test_data['400']\n",
    "smaller_testdata = {'400': a}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1FvL1j1WVtt"
   },
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    correct_count, total_count = evaluate_all(run_pipe, smaller_testdata, display_miss=True, display_correct=False)\n",
    "    print(correct_count/total_count)\n",
    "    print(correct_count)\n",
    "    print(total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFpypUwnQuvV"
   },
   "source": [
    "## Run Test Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MPVyjr8jQ18V",
    "outputId": "20222413-9065-4049-cb93-e347b72f85e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfycTxShQyJB"
   },
   "outputs": [],
   "source": [
    "def run_test_question(input_path, output_path, filter_fn):\n",
    "    questions = str()\n",
    "    with open(input_path, 'r') as file:\n",
    "        questions = file.read()\n",
    "        file.close()\n",
    "    questions = sent_tokenize(questions)\n",
    "\n",
    "    answer_ids = list()\n",
    "    answer_texts = list()\n",
    "    for que in questions:\n",
    "        ans = filter_fn(que)\n",
    "        answer_ids.append(ans[0][0])\n",
    "        answer_texts.append(ans[0][1])\n",
    "    \n",
    "    data = {'question':questions, 'support_id':answer_ids, 'support_sent':answer_texts}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    df.to_csv(output_path)\n",
    "    print(f\"done, result in {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1EzptWlUY04d"
   },
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    run_test_question(test_input_path, test_output_path, run_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eI5C8cFDoQqQ"
   },
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_q8775ooRvt"
   },
   "outputs": [],
   "source": [
    "'''-------Task 1---------'''\n",
    "filePath=\"../data/articles/articles/\"\n",
    "articleSentenceList, articleSentenceList2, articleTokenList, articleTokenList2 = Tokenize(filePath)\n",
    "LemmaList = Lemmatize(articleTokenList2)\n",
    "outputSentenceAndWord(articleSentenceList2,articleTokenList2,LemmaList)\n",
    "#outputDependencyParse(articleSentenceList2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "euae6abPohBV",
    "outputId": "57648a46-43c3-44c7-c4bd-de0508c965c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done, result in ./test_questions.xlsx\n"
     ]
    }
   ],
   "source": [
    "'''-------Task 2--------'''\n",
    "input_path = './sample_questions.txt'\n",
    "output_path = './test_questions.xlsx'\n",
    "run_test_question(input_path, output_path, filter_fn=run_pipe)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "PvcS1o0IpL4z",
    "7vYaKkJDhmJw",
    "iiL2J09mo1Kj",
    "StMirKPaYzU7",
    "4i_sOHui5VV5",
    "YJ5LdGI246ga",
    "GdEPIypw6ZEA",
    "KxyumxoGVxjG",
    "H2TVFmkfYTbt",
    "tFpypUwnQuvV"
   ],
   "name": "QASystem.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "037e485d16884d30a9ec25a8e976bfb7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c8837e4ac0345caa20e3ff93bbbe6be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1f30b1f6ae414236a899cda0a184d803": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c32a8552ff2492e9db7a226c883788d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_92bf43b0404f4f88bd543b6704a08723",
       "IPY_MODEL_496759ed50164754953a0fd092e87b3c",
       "IPY_MODEL_92234c43f8a74e2e910a67d30c61ec3f"
      ],
      "layout": "IPY_MODEL_a1c148714c3447829c21d63049880cfd"
     }
    },
    "34aa90b247ec4700921373d2eaebadc5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3dc2f1be451a475db3d0fc229a183a86": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4379e2f50d1f494e8e8d89588b18f769": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "496759ed50164754953a0fd092e87b3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d823635fa34448b0afc1ef3f648069f2",
      "max": 30,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4379e2f50d1f494e8e8d89588b18f769",
      "value": 30
     }
    },
    "49db0f093d8f495caa46db778134a01f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_037e485d16884d30a9ec25a8e976bfb7",
      "max": 7060,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_74de64df10cb488d9058349f6a323559",
      "value": 7060
     }
    },
    "52e6e3726e1740eca31a49813d5889d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b87efd93515944a0a5f8b9efe0846e52",
      "placeholder": "​",
      "style": "IPY_MODEL_c99059aef8e14ee6981fd4e96586b0ec",
      "value": " 7060/7060 [01:04&lt;00:00, 124.73it/s]"
     }
    },
    "74de64df10cb488d9058349f6a323559": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "92234c43f8a74e2e910a67d30c61ec3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b77f1ed1c4ca4d0380544ce34261796a",
      "placeholder": "​",
      "style": "IPY_MODEL_1c8837e4ac0345caa20e3ff93bbbe6be",
      "value": " 30/30 [00:01&lt;00:00, 31.68it/s]"
     }
    },
    "92bf43b0404f4f88bd543b6704a08723": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f30b1f6ae414236a899cda0a184d803",
      "placeholder": "​",
      "style": "IPY_MODEL_fd77bfbc94404330b591f2deb82ef463",
      "value": "100%"
     }
    },
    "a1c148714c3447829c21d63049880cfd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b77f1ed1c4ca4d0380544ce34261796a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b87efd93515944a0a5f8b9efe0846e52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd9aa1faa2934e3b9ed4219de5a3c4cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c99059aef8e14ee6981fd4e96586b0ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d823635fa34448b0afc1ef3f648069f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da7a43c73c994dad95efe772e3aaf5a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34aa90b247ec4700921373d2eaebadc5",
      "placeholder": "​",
      "style": "IPY_MODEL_bd9aa1faa2934e3b9ed4219de5a3c4cf",
      "value": "100%"
     }
    },
    "ec63c390d00c43f181884fe99d9cee18": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_da7a43c73c994dad95efe772e3aaf5a9",
       "IPY_MODEL_49db0f093d8f495caa46db778134a01f",
       "IPY_MODEL_52e6e3726e1740eca31a49813d5889d4"
      ],
      "layout": "IPY_MODEL_3dc2f1be451a475db3d0fc229a183a86"
     }
    },
    "fd77bfbc94404330b591f2deb82ef463": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
